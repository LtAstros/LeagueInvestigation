{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ASSIGNMENT CONFIG\n",
    "files:\n",
    "    - data/login_table.csv\n",
    "    - data/payment.csv\n",
    "    - data/sales.csv\n",
    "    - data/skittles.tsv\n",
    "    - data/imgs/image_0.png\n",
    "    - data/imgs/image_1.png\n",
    "    - data/imgs/image_2.png\n",
    "    - data/imgs/image_3.png   \n",
    "    - util.py\n",
    "autograder_files:\n",
    "environment: ./environment.yaml\n",
    "requirements:\n",
    "    - otter-grader==3.1.4\n",
    "generate:\n",
    "    show_hidden: true\n",
    "    public_multiplier: 1\n",
    "init_cell: true\n",
    "export_cell: false\n",
    "overwrite_requirements: true\n",
    "run_tests: false\n",
    "template_pdf: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 ‚Äì Pivot Tables, Permutation Testing, and Missing Values\n",
    "\n",
    "## DSC 80, Fall 2022\n",
    "\n",
    "### Due Date: Monday, October 24th at 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying `lab.py` file will be tested (a la DSC 20),\n",
    "2. The notebook may be graded (if it contains free response questions or asks you to draw plots).\n",
    "\n",
    "**Do not change the function names in the `*lab.py` file!**\n",
    "- The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name.\n",
    "- If you changed something you weren't supposed to, just use git to revert! Ask us if you need help with this, or google around for `git revert`.\n",
    "\n",
    "**Tips for working in the notebook**:\n",
    "- The notebooks serve to present the questions and give you a place to present your results for later review.\n",
    "- The notebooks in *lab assignments* are not graded (only the `lab.py` file is submitted and graded).\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file.\n",
    "\n",
    "**Tips for developing in the `lab.py` file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional helper functions to solve the lab! \n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab.py`\n",
    "\n",
    "* We import our `lab.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from datetime import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 0 ‚Äì Mid-Quarter Survey üôã\n",
    "\n",
    "Course staff, along with the Data Science Student Representatives, have put together a mid-quarter survey that will allow you to share feedback on your experience in DSC 80 so far. It is **entirely anonymous**, so we encourage you to be honest.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><b><a href=https://forms.gle/5Bd64352PArTH7Z89>Click here to access the mid-quarter survey.</a></b></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "We'd like to have as many students as possible in the class fill out the survey. **As such, if 80% of the class fills out this survey before the Midterm Exam, then everyone will earn an extra point on the Midterm Exam.** The survey will close before the Midterm Exam. \n",
    "\n",
    "We really appreciate your feedback, thanks! üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Time Series Data\n",
    "\n",
    "Imagine that you own an online store and you'd like to monitor the visits to your site. You've collected some data that you store in `data/login_table.csv`. It contains the information about different login dates and times for different users. Some users are unique, some visited your store multiple times.\n",
    "\n",
    "You need to answer a few questions below in order to understand the login patters of your users."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 ‚Äì Passwords üîë\n",
    "\n",
    "Write a function `latest_login` which takes in a DataFrame like `login` and outputs a DataFrame indexed by `'Login Id'`, counting the number of logins that occurs at the prime time for each user. Prime time is as it says: from 4 PM to 8 PM (inclusive). The DataFrame should have just one column, named `'Time'`.\n",
    "\n",
    "For example, if a user logs in at 5 PM one day, 1 PM another day, and again at 8 PM, then her total number of prime-time log-ins is 2. Note that the values in your returned DataFrame should only include counts, not datetime objects.\n",
    "\n",
    "***Note:*** You do not need to use Python's `datetime` module ‚Äì instead, use the built-in `pandas` methods for working with times that we introduced in [Lecture 6](https://github.com/dsc-courses/dsc80-2022-fa/blob/main/lectures/06-combining/notebook/lecture.ipynb) (though you will need to do a bit more research to fully answer the question). Do not use a `for`-loop."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def latest_login(login):\n",
    "    \"\"\"Calculates the latest login time for each user\n",
    "    :param login: a DataFrame with login information\n",
    "    :return: a DataFrame with latest login time for\n",
    "    each user indexed by \"Login Id\"\n",
    "    >>> fp = os.path.join('data', 'login_table.csv')\n",
    "    >>> login = pd.read_csv(fp)\n",
    "    >>> result = latest_login(login)\n",
    "    >>> len(result)\n",
    "    433\n",
    "    >>> result.loc[381, \"Time\"] > 7\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    login_time = login.copy()\n",
    "    login_time[\"Time\"] = pd.to_datetime(login_time[\"Time\"])\n",
    "    counts = login_time.groupby('Login Id').agg(lambda ser: sum((ser.dt.hour >= 16) & (ser.dt.hour <= 20)))\n",
    "    return counts\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = os.path.join('data', 'login_table.csv')\n",
    "login = pd.read_csv(fp)\n",
    "q1_result = latest_login(login)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "\"\"\" # END TEST CONFIG\n",
    "len(q1_result) == 433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q1_result.loc[381, \"Time\"] > 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message:  You should not use loops in Pandas. Loop was found in latest_login.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import util\n",
    "not util.check_loops(latest_login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "failure_message:  'check user 457'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q1_result.loc[457, 'Time'] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "failure_message:  'check user 457'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q1_result.loc[458, 'Time'] > 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message:  'check number of unique users'\n",
    "\"\"\" # END TEST CONFIG\n",
    "len(set(q1_result.index)) == 433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: .5\n",
    "failure_message:  'check user 381'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q1_result.loc[381, 'Time'] == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "failure_message:  'check user 381'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q1_result.loc[381, 'Time'] > 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'check the entire list of times'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q1_result.loc[:, 'Time'].mean(), 2.127, atol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'median hour: approx'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q1_result.loc[:, 'Time'].median(), 1, atol=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'median hour: approx'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q1_result.loc[:, 'Time'].median(),1, atol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'check the minimum number of logins'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q1_result.loc[:, 'Time'].min() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'check the maximum number of logins'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q1_result.loc[:, 'Time'].max() == 67"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 ‚Äì Return Users üîÅ\n",
    "\n",
    "As a site owner, you would like to find your most enthusiastic users -- the ones who return to your site most frequently. You've noticed that there are users who have several logins and users who logged in only once. You are interested in finding the number of logins *per day* for each user.\n",
    "\n",
    "To do this, you can assume that today is  January 5, 2018. The first login date of users is the first day of their membership in the site, and you can assume that they are still a member today. For simplicity, you only need to count full days that a user has been a member. For example, if a user's first login was 12 days and 5 hours ago, you can say that they have been a user for 12 days. \n",
    "\n",
    "Write a function `calculate_frequency` which takes in a DataFrame like `login` and outputs a Series containing the number of logins per day for each user. Your Series should have `'Login Id'`s in its index, and the frequencies as its values. The order of users in the index is arbitrary.\n",
    "\n",
    "**IMPORTANT**: No Loops Allowed.\n",
    "\n",
    "***Hint:*** Can you write a custom aggregator that allows you to do this with just one `.groupby`?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequency(login):\n",
    "    \"\"\"\n",
    "    Calculates the the login frequency for each user.\n",
    "    :param login: a DataFrame with login information but without unique IDs\n",
    "    :return: a Series, indexed by Login ID, containing \n",
    "    the login frequency for each user.\n",
    "    >>> fp = os.path.join('data', 'login_table.csv')\n",
    "    >>> login = pd.read_csv(fp)\n",
    "    >>> freq = count_frequency(login)\n",
    "    >>> len(freq)\n",
    "    433\n",
    "    >>> np.isclose(freq.loc[466], 0.24517906336088155)\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    login = login.copy()\n",
    "    login['Time'] = pd.to_datetime(login['Time'])\n",
    "    \n",
    "    def login_frequency(ser):\n",
    "        days = (pd.Timestamp('2018-01-05 23:59') - ser.min()).days\n",
    "        # number of logins divided by number of days\n",
    "        return len(ser) / days\n",
    "\n",
    "    return login.groupby('Login Id')['Time'].aggregate(login_frequency)\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = os.path.join('data', 'login_table.csv')\n",
    "login = pd.read_csv(fp)\n",
    "q2_result = count_frequency(login)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "\"\"\" # END TEST CONFIG\n",
    "len(q2_result) == 433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q2_result.loc[466], 0.24517906336088155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message:  You should not use loops in Pandas. Loop was found in count_frequency.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import util\n",
    "not util.check_loops(count_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'max frequency in table'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q2_result.max() == 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'number of people whose frequency is greater than 0.5'\n",
    "\"\"\" # END TEST CONFIG\n",
    "(q2_result > 0.5).sum() == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'number of people whose frequency is less than 0.1'\n",
    "\"\"\" # END TEST CONFIG\n",
    "(q2_result < 0.1).sum() == 383"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'number of people whose frequency is between 0.2 and 0.4'\n",
    "\"\"\" # END TEST CONFIG\n",
    "((q2_result > 0.2) & (q2_result < 0.4)).sum() == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'number of people whose frequency is between 0.3 and 0.6'\n",
    "\"\"\" # END TEST CONFIG\n",
    "((q2_result > 0.3) & (q2_result < 0.6)).sum() == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'min frequency: approx'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q2_result.min(), 0.0027624309392265192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'number of people whose frequency is between 0.7 and 0.8'\n",
    "\"\"\" # END TEST CONFIG\n",
    "((q2_result > 0.7) & (q2_result < 0.8)).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'check Login Id 1307'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q2_result.loc[1307] == 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Pivot Tables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 ‚Äì Summarizing Sales üí∞\n",
    "\n",
    "Recall from [Lecture 5](https://github.com/dsc-courses/dsc80-2022-fa/blob/main/lectures/05-grouping/notebook/lecture.ipynb), a pivot table allows you to aggregate the entries in a DataFrame based on two categorical columns. In this question, you are given a simple dataset, `data/sales.csv`, and are asked to solve a few simple problems using the `pivot_table` method.  \n",
    "\n",
    "We have provided the outline for your DataFrames, but yours may have a different number of rows and columns and different values.\n",
    "\n",
    "#### `total_seller`\n",
    "\n",
    "Write a function `total_seller` that takes in the `sales` DataFrame and returns a DataFrame that contains the total sales for each seller, indexed by `'Name'`. There should not be any `NaN`s.\n",
    "\n",
    "***Note:*** You may be able to implement `total_seller` without using `pivot_table`.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Total</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Name</th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Jones</th>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Smith</th>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Trump</th>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `product_name`\n",
    "\n",
    "Write a function `product_name` that takes in the `sales` DataFrame and returns a DataFrame that contains the total sales for each seller, indexed by `'Product'`. Do not fill in `NaN`s.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Name</th>\n",
    "      <th>Jones</th>\n",
    "      <th>Smith</th>\n",
    "      <th>Trump</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Product</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>boat</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>book</th>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>hotel</th>\n",
    "      <td>NaN</td>\n",
    "      <td>NaN</td>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>pen</th>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ruler</th>\n",
    "      <td>0.0</td>\n",
    "      <td>0.0</td>\n",
    "      <td>NaN</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `count_product`\n",
    "\n",
    "Write a function `count_product` that takes in the `sales` DataFrame and returns a DataFrame that contains the total number of items sold product-wise and name-wise per date. Replace `NaN`s with 0s. Don't reset the index after pivoting.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Date</th>\n",
    "      <th>01.01.2012</th>\n",
    "      <th>02.20.2013</th>\n",
    "      <th>02.25.2015</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Product</th>\n",
    "      <th>Name</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>boat</th>\n",
    "      <th>Trump</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"3\" valign=\"top\">book</th>\n",
    "      <th>Jones</th>\n",
    "      <td>0</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Smith</th>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Trump</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>hotel</th>\n",
    "      <th>Trump</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `total_by_month`\n",
    "\n",
    "Write a function `total_by_month` that takes in the `sales` DataFrame and returns a pivot table that contains the total sales name-wise, product-wise per month. Replace `NaN`s with 0s. Don't reset the index after pivoting.\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Month</th>\n",
    "      <th>February</th>\n",
    "      <th>January</th>\n",
    "      <th>July</th>\n",
    "      <th>March</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>Name</th>\n",
    "      <th>Product</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th rowspan=\"3\" valign=\"top\">Jones</th>\n",
    "      <th>book</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>pen</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ruler</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th rowspan=\"3\" valign=\"top\">Smith</th>\n",
    "      <th>book</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>pen</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>ruler</th>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***Note:*** [Here](https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html) is another great resource that provides an overview of `pivot_table` with many examples from the Titanic dataset."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def total_seller(sales):\n",
    "    \"\"\"\n",
    "    total_seller should take in the sales DataFrame and \n",
    "    return a DataFrame that contains the total sales \n",
    "    for each seller, indexed by 'Name'. There should not be any NaNs.\n",
    "\n",
    "    >>> fp = os.path.join('data', 'sales.csv')\n",
    "    >>> sales = pd.read_csv(fp)\n",
    "    >>> out = total_seller(sales)\n",
    "    >>> out.shape[0]\n",
    "    3\n",
    "    >>> out[\"Total\"].sum() < 15000\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    result = sales.pivot_table(index=\"Name\", values=\"Total\", aggfunc='sum')\n",
    "    return result\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def product_name(sales):\n",
    "    \"\"\"\n",
    "    product_name should take in the sales DataFrame and \n",
    "    return a DataFrame that contains the total sales \n",
    "    for each seller, indexed by 'Product'. \n",
    "    Do not fill in NaNs.\n",
    "    \n",
    "    >>> fp = os.path.join('data', 'sales.csv')\n",
    "    >>> sales = pd.read_csv(fp)\n",
    "    >>> out = product_name(sales)\n",
    "    >>> out.size\n",
    "    15\n",
    "    >>> out.loc[\"pen\"].isnull().sum()\n",
    "    0\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    result = sales.pivot_table(\n",
    "        index=\"Product\",\n",
    "        columns=\"Name\",\n",
    "        values=\"Total\",\n",
    "        aggfunc='sum'\n",
    "    )\n",
    "    return result\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def count_product(sales):\n",
    "    \"\"\"\n",
    "    count_product should take in the sales DataFrame and \n",
    "    return a DataFrame that contains the total number of \n",
    "    items sold product-wise and name-wise per date. \n",
    "    Replace NaNs with 0s.\n",
    "\n",
    "    >>> fp = os.path.join('data', 'sales.csv')\n",
    "    >>> sales = pd.read_csv(fp)\n",
    "    >>> out = count_product(sales)\n",
    "    >>> out.loc[\"boat\"].loc[\"Trump\"].value_counts()[0]\n",
    "    6\n",
    "    >>> out.size\n",
    "    70\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    result = sales.pivot_table(\n",
    "        index=[\"Product\",\"Name\"],\n",
    "        columns=\"Date\",\n",
    "        values=\"Total\",\n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    return result\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def total_by_month(sales):\n",
    "    \"\"\"\n",
    "    total_by_month should take in the sales DataFrame \n",
    "    and return a pivot table that contains the total \n",
    "    sales name-wise, product-wise per month. \n",
    "    Replace NaNs with 0s.\n",
    "    \n",
    "    >>> fp = os.path.join('data', 'sales.csv')\n",
    "    >>> sales = pd.read_csv(fp)\n",
    "    >>> out = total_by_month(sales)\n",
    "    >>> out[\"May\"].idxmax()\n",
    "    ('Smith', 'book')\n",
    "    >>> out.shape[1]\n",
    "    5\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    sales = sales.copy()\n",
    "    sales['Date']=pd.to_datetime(sales['Date'])\n",
    "    sales['Month'] = sales['Date'].apply(lambda x:x.month_name())\n",
    "    result = sales.pivot_table(\n",
    "        index =[\"Name\",\"Product\"],\n",
    "        columns=\"Month\",\n",
    "        values=\"Total\",\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    return result\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = os.path.join('data', 'sales.csv')\n",
    "sales = pd.read_csv(fp)\n",
    "q3_total_seller_out = total_seller(sales)\n",
    "q3_product_name_out = product_name(sales)\n",
    "q3_product_count_out = count_product(sales)\n",
    "q3_total_by_month_out = total_by_month(sales)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_total_seller_out.shape[0] == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_total_seller_out[\"Total\"].sum() < 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_product_name_out.size == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_product_name_out.loc[\"pen\"].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_product_count_out.loc[\"boat\"].loc[\"Trump\"].value_counts()[0] == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_product_count_out.size == 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_total_by_month_out[\"May\"].idxmax() == ('Smith', 'book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_total_by_month_out.shape[1] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 0.25\n",
    "failure_message:  You should not use loops in Pandas. Loop was found in total_seller.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import util\n",
    "not util.check_loops(total_seller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 0.25\n",
    "failure_message:  You should not use loops in Pandas. Loop was found in product_name.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import util\n",
    "not util.check_loops(product_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 0.25\n",
    "failure_message:  You should not use loops in Pandas. Loop was found in count_product.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import util\n",
    "not util.check_loops(count_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 0.25\n",
    "failure_message:  You should not use loops in Pandas. Loop was found in total_by_month.\n",
    "\"\"\" # END TEST CONFIG\n",
    "import util\n",
    "not util.check_loops(total_by_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'Smith Total'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_total_seller_out.loc['Smith', 'Total'] == 6800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'Jones Total'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_total_seller_out.loc['Jones', 'Total'] == 3680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'boat / Trump'\n",
    "\"\"\" # END TEST CONFIG\n",
    "int(q3_product_name_out.loc['boat', 'Trump']) == 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'ruler / Smith'\n",
    "\"\"\" # END TEST CONFIG\n",
    "int(q3_product_name_out.loc['ruler', 'Smith']) == 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'null values: hotel row'\n",
    "\"\"\" # END TEST CONFIG\n",
    "int(q3_product_name_out.loc['hotel'].isnull().sum()) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'correct shape'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q3_product_count_out.shape == (10, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'Total number of 0 entries.'\n",
    "\"\"\" # END TEST CONFIG\n",
    "(q3_product_count_out == 0).sum().sum() == 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'books for smith in May'\n",
    "\"\"\" # END TEST CONFIG\n",
    "int(q3_total_by_month_out.loc[('Smith', 'book'), 'May']) == 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "failure_message: 'number of entries strictly over 2000'\n",
    "\"\"\" # END TEST CONFIG\n",
    "(q3_total_by_month_out > 2000).sum().sum() == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: .5\n",
    "failure_message: 'number of zero entries'\n",
    "\"\"\" # END TEST CONFIG\n",
    "(q3_total_by_month_out == 0).sum().sum() == 39"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Permutation Testing\n",
    "\n",
    "[Skittles](https://en.wikipedia.org/wiki/Skittles_(confectionery)) üç¨ are made in two locations in the United States: Yorkville, Illinois and Waco, Texas. In these factories, Skittles of different colors are made separately by different machines and combined/packaged into bags for sale. The **tab-separated file** `data/skittles.tsv` contains the contents of 468 bags of Skittles.\n",
    "\n",
    "Throughout this question, we will compare the color distribution of Skittles between bags made in the Yorkville factory and bags made in the Waco factory. Most people have preferences for their favorite flavor and there is a surprising amount of variation among the distribution of flavors in each bag.\n",
    "\n",
    "Look at the variation by bag in the dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red</th>\n",
       "      <th>orange</th>\n",
       "      <th>yellow</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>Factory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   red  orange  yellow  green  purple    Factory\n",
       "0   10      15      11      7      18  Yorkville\n",
       "1    5      12      17     15      10  Yorkville\n",
       "2   16      11      15     11       9       Waco\n",
       "3   15       8      13     16       7       Waco\n",
       "4   11      14      20      8       7       Waco"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skittles_fp = os.path.join('data', 'skittles.tsv')\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\t')\n",
    "skittles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 6)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skittles.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 ‚Äì Orange Skittles üü†\n",
    "\n",
    "First, you will investigate if the machine that mixes together the Skittles of different colors might favor one color over another. Use a permutation test to assess whether, on average, bags made in Yorkville have the same number of orange skittles as bags made in Waco. Do this by implementing the functions described below.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `diff_of_median_proportions`\n",
    "\n",
    "Create a function `diff_of_median_proportions` that takes in a DataFrame like `skittles` and returns the **absolute difference** between the **median proportion** of orange Skittles per bag from Yorkville and the **median proportion** of orange Skittles per bag from Waco.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `simulate_null`\n",
    "\n",
    "Create a function `simulate_null` that takes in a DataFrame like `skittles` and returns one simulated instance of the test statistic under the null hypothesis. Note that this will involve shuffling!\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `pval_color`\n",
    "\n",
    "Create a function `pval_color` that takes in a DataFrame like `skittles` and calculates the p-value for the permutation test using 1000 trials.\n",
    "\n",
    "<br>\n",
    "\n",
    "Plot the observed statistic, along with the histogram for the simulated distribution, to check your work.\n",
    "\n",
    "***Note:*** In all functions, the default argument for `col` is `'orange'`. Your functions should still work for any color so that you can call it in later questions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def diff_of_median_proportions(data, col='orange'):\n",
    "    \"\"\"\n",
    "    diff_of_median_proportions takes in a DataFrame like skittles \n",
    "    and returns the absolute difference of median proportions \n",
    "    between the number of oranges per bag from Yorkville and Waco.\n",
    "    :Example:\n",
    "    >>> skittles_fp = os.path.join('data', 'skittles.tsv')\n",
    "    >>> skittles = pd.read_csv(skittles_fp, sep='\\\\t')\n",
    "    >>> out = diff_of_median_proportions(skittles)\n",
    "    >>> 0 <= out\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    data['total_skittles'] = data.sum(axis=1,numeric_only=True) # Finding total number of skittles in the current bag\n",
    "    data[col+'_proportion'] = data.apply(lambda row : row[col]/row['total_skittles'],axis=1)\n",
    "    out = data.groupby('Factory')[col+'_proportion'].median().diff().abs().iloc[-1]\n",
    "    data.drop(col+'_proportion',axis=1,inplace=True)\n",
    "    data.drop('total_skittles',axis=1,inplace=True)\n",
    "    return out\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def simulate_null(data, col='orange'):\n",
    "    \"\"\"\n",
    "    simulate_null takes in a DataFrame like skittles and \n",
    "    returns one simulated instance of the test statistic \n",
    "    under the null hypothesis.\n",
    "    :Example:\n",
    "    >>> skittles_fp = os.path.join('data', 'skittles.tsv')\n",
    "    >>> skittles = pd.read_csv(skittles_fp, sep='\\\\t')\n",
    "    >>> out = simulate_null(skittles)\n",
    "    >>> isinstance(out, float)\n",
    "    True\n",
    "    >>> 0 <= out <= 1.0\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    shuffled = data[col].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "    dg = data.assign(shuffled=shuffled)\n",
    "    stat = diff_of_median_proportions(dg, col='shuffled')\n",
    "    return stat\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "def pval_color(data, col='orange'):\n",
    "    \"\"\"\n",
    "    pval_color takes in a DataFrame like skittles and \n",
    "    calculates the p-value for the permutation test \n",
    "    using 1000 trials.\n",
    "    \n",
    "    :Example:\n",
    "    >>> skittles_fp = os.path.join('data', 'skittles.tsv')\n",
    "    >>> skittles = pd.read_csv(skittles_fp, sep='\\\\t')\n",
    "    >>> pval = pval_color(skittles)\n",
    "    >>> isinstance(pval, float)\n",
    "    True\n",
    "    >>> 0 <= pval <= 0.1\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    results = []\n",
    "    for _ in range(1000):\n",
    "        results.append(simulate_null(data, col))\n",
    "        \n",
    "    obs = diff_of_median_proportions(data, col)\n",
    "    pval = (pd.Series(results) >= obs).mean()\n",
    "    \n",
    "    return pval\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "# cell may take about 1-2 minutes to execute to completion\n",
    "skittles_fp = os.path.join('data', 'skittles.tsv')\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\\\t', engine='python')\n",
    "q4_diff_of_median_proportions_out = diff_of_median_proportions(skittles)\n",
    "q4_simulate_null_out = simulate_null(skittles)\n",
    "q4_many_diffs = np.array([simulate_null(skittles) for _ in range(100)])\n",
    "q4_pval_out = pval_color(skittles)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "0 <= q4_diff_of_median_proportions_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "isinstance(q4_simulate_null_out, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "0 <= q4_simulate_null_out <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "isinstance(q4_pval_out, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "0 <= q4_pval_out <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'greater than zero'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q4_diff_of_median_proportions_out > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'answer: approx within 0.01'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q4_diff_of_median_proportions_out, 0.008705483323911828, atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'answer: approx within 0.001'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q4_diff_of_median_proportions_out,0.008705483323911828, atol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'greater than zero'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q4_simulate_null_out >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'answer: mean null distribution near 0.00004'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q4_many_diffs.mean(), 0.00482790384428229, atol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'orange p-value: doctest'\n",
    "\"\"\" # END TEST CONFIG\n",
    "0 <= q4_pval_out <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'orange p-value: within 0.05'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q4_pval_out, 0.113, atol=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'orange p-value: approx: within 0.075'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q4_pval_out, 0.113, atol=0.075)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'orange p-value: approx'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q4_pval_out, 0.113, atol=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 ‚Äì Generalizing to all colors üî¥üü†üü°üü¢üü£\n",
    "\n",
    "While your `pval_color` function used a default color of `'orange'`, it should also work for all other colors of Skittles, meaning you can run the same permutation test from Question 4 on all colors of Skittles. Call `pval_color` on all colors of Skittles to find which colors differ the most between the two locations on average. \n",
    "\n",
    "Then, create a function `ordered_colors` that returns a list of five ordered pairs, each of the form `('color', p_value)`. For example, your list might look like `[('pink', 0.000), ('brown', 0.025), ...]`. \n",
    "\n",
    "The list should be **hard-coded**, meaning that you should run your permutation tests in your notebook, not in your `.py` file. The list should also be sorted in **increasing order of p-value**. Make sure your p-values are rounded to **3 decimal places**.\n",
    "\n",
    "Even though there is randomness in the color composition in each bag, this list gives the likelihood that the machines have a systematic, meaningful, difference in how they blend the colors in each bag."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def ordered_colors():\n",
    "    \"\"\"\n",
    "    ordered_colors returns your answer as an ordered\n",
    "    list from \"most different\" to \"least different\" \n",
    "    between the two locations. You list should be a \n",
    "    hard-coded list, where each element has the \n",
    "    form (color, p-value).\n",
    "\n",
    "    :Example:\n",
    "    >>> out = ordered_colors()\n",
    "    >>> len(out) == 5\n",
    "    True\n",
    "    >>> colors = {'green', 'orange', 'purple', 'red', 'yellow'}\n",
    "    >>> set([x[0] for x in out]) == colors\n",
    "    True\n",
    "    >>> all([isinstance(x[1], float) for x in out])\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    L = []\n",
    "    for color in skittles.columns[:-1]:\n",
    "        p = pval_color(skittles, col=color)\n",
    "        L.append((color, p))\n",
    "\n",
    "    out = sorted(L, key=lambda x:x[1])\n",
    "    return out\n",
    "\n",
    "    return out\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q5_out = ordered_colors()\n",
    "q5_colors = {'green', 'orange', 'purple', 'red', 'yellow'}\n",
    "q5_test_colors = [x[0] for x in q5_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yellow', 0.0),\n",
       " ('red', 0.061),\n",
       " ('orange', 0.107),\n",
       " ('purple', 0.337),\n",
       " ('green', 1.0)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q5_out"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "len(q5_out) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "set([x[0] for x in q5_out]) == q5_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "\"\"\" # END TEST CONFIG\n",
    "all([isinstance(x[1], float) for x in q5_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'yellow less than orange'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q5_test_colors.index('orange') > q5_test_colors.index('yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'orange less than red'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q5_test_colors.index('orange') > q5_test_colors.index('red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'red greater than or equal to green'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q5_test_colors.index('green') > q5_test_colors.index('red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'green less than purple'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q5_test_colors.index('purple') < q5_test_colors.index('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'smallest pval'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q5_out[0][1], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: '2nd pval'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q5_out[1][1], 0.059, atol=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: '3rd pval'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q5_out[2][1], 0.095, atol=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: '4th pval'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q5_out[3][1], 0.302, atol=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'largest pval'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q5_out[4][1], 0.998, atol=0.02)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 ‚Äì Overall distributions üè≠\n",
    "\n",
    "Now, suppose you would like to assess whether the two locations make similar amounts of each color overall. That is, suppose we:\n",
    "* Combine and count up all the Skittles of each color that were made in Yorkville (e.g. 14303 total red skittles, 9091 total green skittles, etc.)\n",
    "* Combine and count up all the Skittles of each color that were made in Waco.\n",
    "\n",
    "**Are these distributions of colors similar?** Is the variation among the bags due to each factory making different amounts of each color?\n",
    "\n",
    "Use a permutation test to assess whether the distribution of colors of Skittles made in Yorkville is statistically significantly different than those made in Waco. Set a significance level of 0.01 and determine whether you can reject a null hypothesis that answers the question above using a permutation test with 1000 trials. For your test statistic, use the **total variation distance (TVD)**.\n",
    "\n",
    "Refer to [Lecture 7](https://github.com/dsc-courses/dsc80-2022-fa/blob/main/lectures/07-permutation/notebook/lecture.ipynb) to see an example of a [permutation test](https://www.inferentialthinking.com/chapters/12/Comparing_Two_Samples.html) that uses the [TVD](https://inferentialthinking.com/chapters/11/2/Multiple_Categories.html) as the test statistic. Some guidance:\n",
    "\n",
    "- Our previous permutation tests have compared the median proportion of (say) orange Skittles in Yorkville bags to the median proportion number of orange Skittles in Waco bags. The role of shuffling was to randomly assign bags to Yorkville and Waco.\n",
    "- In this permutation test, we are **still** shuffling to randomly assign bags to Yorkville and Waco. The only difference is that after we randomly assign each bag to a factory, we will compute the distribution of colors amongst the two factories and find the TVD between those two distributions.\n",
    "\n",
    "**Your job:** Create a function `same_color_distribution` that takes in no arguments and outputs a hard-coded **tuple** with the p-value and whether you `'Fail to Reject'` or `'Reject'` the null hypothesis."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION NO PROMPT\n",
    "# helper function\n",
    "def tvd(data):\n",
    "    sums = data.groupby('Factory').sum()\n",
    "    distrs = sums.div(sums.sum(axis=1), axis=0)\n",
    "    tvd = distrs.diff().iloc[-1].abs().sum() / 2\n",
    "    \n",
    "    return tvd\n",
    "\n",
    "\n",
    "# helper function\n",
    "def permtest(data):\n",
    "    N = 1000\n",
    "    results = []\n",
    "    dg = data.copy()\n",
    "    for _ in range(N):\n",
    "        dg['Factory'] = (\n",
    "            dg['Factory']\n",
    "            .sample(frac=1, replace=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        results.append(tvd(dg))\n",
    "\n",
    "    obs = tvd(data)\n",
    "    pval = (pd.Series(results) >= obs).mean()\n",
    "    \n",
    "    return pval\n",
    "    \n",
    "    # END SOLUTION\n",
    "\n",
    "def same_color_distribution():\n",
    "    \"\"\"\n",
    "    same_color_distribution outputs a hard-coded tuple \n",
    "    with the p-value and whether you 'Fail to Reject' or 'Reject' \n",
    "    the null hypothesis.\n",
    "\n",
    "    >>> out = same_color_distribution()\n",
    "    >>> isinstance(out, tuple)\n",
    "    True\n",
    "    >>> isinstance(out[0], float)\n",
    "    True\n",
    "    >>> out[1] in ['Fail to Reject', 'Reject']\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    out = (0.008, 'Reject')\n",
    "    return out\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q6_out = same_color_distribution()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 0.5\n",
    "failure_message: 'tuple'\n",
    "\"\"\" # END TEST CONFIG\n",
    "isinstance(q6_out, tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'wrong output type at index 0'\n",
    "\"\"\" # END TEST CONFIG\n",
    "isinstance(q6_out[0], float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'wrong output type at index 1'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q6_out[1] in ['Fail to Reject', 'Reject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'reject the null hypothesis'\n",
    "\"\"\" # END TEST CONFIG\n",
    "cond1 = q6_out[1] == 'Reject'\n",
    "cond2 = (q6_out[0] > 0.01) and (q6_out[1] == 'Fail to Reject')\n",
    "cond1 or cond2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'p-value, approximate within 0.1'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q6_out[0], 0.005, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'p-value, approximate within 0.05'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q6_out[0], 0.005, atol=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'p-value, approximate within 0.01'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q6_out[0], 0.005, atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'p-value, approximate within 0.25'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q6_out[0], 0.005, atol=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'p-value, approximate within 0.5'\n",
    "\"\"\" # END TEST CONFIG\n",
    "np.isclose(q6_out[0], 0.005, atol=0.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 ‚Äì Permutation testing vs. hypothesis testing üß™\n",
    "\n",
    "In each of the following scenarios, decide  whether  a  permutation test is appropriate to determine if there is a  significant difference between the quantities described. If a permutation test is appropriate, mark `'P'`. Otherwise, mark `'H'`.\n",
    "\n",
    "Record your answers in the function `perm_vs_hyp` that outputs a list of length 5, containing the values `'P'` and `'H'`.\n",
    "\n",
    "1. Compare the DSC 80 pass rate between second years and third years who take the class.\n",
    "2. Compare the proportion of Data Science majors who have completed DSC 80 and the proportion of Data Science minors who have completed DSC 80.\n",
    "3. Compare the proportion of students who have iPhones to the proportion of students who have Android phones (for simplicity, assume that all students either have an iPhone or an Android).\n",
    "4. In DSC 80, we ask all students whether they liked DSC 40A or DSC 40B more. Compare the proportion of students who preferred DSC 40A to the proportion who preferred DSC 40B.\n",
    "5. Compare the attendance rate of classes that play music before class vs. classes that do not play music before class.\n",
    "\n",
    "***Hint:*** Think about the type of data you would collect in each case, and how you would simulate new data under the null hypothesis."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def perm_vs_hyp():\n",
    "    \"\"\"\n",
    "    Multiple choice response for Question 8.\n",
    "\n",
    "    >>> out = perm_vs_hyp()\n",
    "    >>> ans = ['P', 'H']\n",
    "    >>> len(out) == 5\n",
    "    True\n",
    "    >>> set(out) <= set(ans)\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    '''\n",
    "    Scenario 1: Permutation test. These are two different groups. \n",
    "    We'd have one list of second years and whether or not they pass, e.g. sophomores = [1, 0, 0, 1, 1, 0, 0],\n",
    "    and another for third years, e.g. juniors = [1, 1, 1, 1, 1, 0, 0, 1].\n",
    "    Under our null, we randomly assign students to be second or third years and look at the differences\n",
    "    in pass rates.\n",
    "    \n",
    "    Scenario 2: Permutation test. Again, these are two different groups.\n",
    "    We'd have one list of DSC majors and whether or not they've completed DSC 80, e.g. majors = [1, 0, 0, 0, 1],\n",
    "    and another for DSC minors, e.g. minors = [0, 0, 0, 1, 1, 1].\n",
    "    Under our null, we randomly assign students to be majors or minors and look at differences\n",
    "    in completion rates.\n",
    "    \n",
    "    Scenario 3: Hypothesis test. This is just one group ‚Äì students in one section of one class, for example.\n",
    "    To simulate data, I essentially flip a coin ‚Äì heads is iPhone, tails is Android.\n",
    "    If the question was, do iPhone users score higher than Android users on average, then we could\n",
    "    run a permutation test, but that's not the question here.\n",
    "    \n",
    "    Scenario 4: Hypothesis test. Same as above, but DSC 40A and DSC 40B instead of iPhone and Android.\n",
    "    \n",
    "    Scenario 5: Permutation test. Two different groups ‚Äì classes that play music before class and classes that\n",
    "    don't. For both groups, we have separate lists of information.\n",
    "    '''\n",
    "\n",
    "    out = ['P', 'P', 'H', 'H', 'P']\n",
    "\n",
    "    return out\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q7_out = perm_vs_hyp()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'output length should be 5'\n",
    "\"\"\" # END TEST CONFIG\n",
    "len(q7_out) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'output contains answers other than P or H'\n",
    "\"\"\" # END TEST CONFIG\n",
    "set(q7_out) <= set(['P', 'H'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 1'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q7_out[0] == 'P' # Don't just copy the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 2'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q7_out[1] == 'P' # Don't just copy the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 3'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q7_out[2] == 'H' # Don't just copy the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 4'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q7_out[3] == 'H' # Don't just copy the answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 5'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q7_out[4] == 'P' # Don't just copy the answer!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Types of Missingness\n",
    "\n",
    "First, let's recap the different mechanisms of missingness we studied in lecture.\n",
    "\n",
    "### Missing by Design (MD)\n",
    "- The missing field is deliberately missing. The missing field is deliberately set to null or not collected (hence, \"missing by design\").\n",
    "- The missingness can be exactly predicted when a column will be null, with only knowledge of the other columns using a function of the rows of the dataset.\n",
    "\n",
    "### Missing Completely at Random (MCAR)\n",
    "- The missingness of missing value isn't related to the actual, unreported value itself, nor the values in any other fields. The missingness is not systematic.\n",
    "- The missingness is unconditionally uniform across rows. MCAR doesn't bias the observed data.\n",
    "- There is no relationship between the missing data and the any of the other data, observed or missing.\n",
    "\n",
    "### Missing at Random (MAR)\n",
    "- The missingness of the missing value has nothing to do with the value itself, but may be related to another field.\n",
    "- The missingness is uniform across rows, perhaps conditional on another column. MAR biases the observed data, but is fixable.\n",
    "- There is a systematic relationship between the missing values and the observed data (but not the missing values themselves).\n",
    "- Difference between MD and MAR: If you can *exactly/always* determine missingness using the other columns, the missingness is MD. If there is just some sort of systematic relationship between the missing columns/values and other columns/values that may help us predict missingness, the missingness is MAR.\n",
    "\n",
    "### Non-Ignorable (NI, aka NMAR)\n",
    "- The missingness of the missing value is related to the actual, unreported value.\n",
    "- NI biases the observed data in unobservable ways.\n",
    "- There is relationship between the propensity of a value to be missing and its value.\n",
    "- ***Note:*** In lecture, we referred to non-ignorable missingness as \"not missing at random (NMAR)\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 ‚Äì After-purchase surveys üõí\n",
    "\n",
    "You run a small e-commerce website and send surveys out to customers after they purchase an item from your store. The survey asks whether the customer is satisfied with their purchase (\"Yes\" or \"No\"). Below, you are presented with possible datasets, each of which contains a column `'satisfied'` as described above, as well as a `'customer_id'` number corresponding to the customer and an `'item'` column describing the item that the customer purchased. **The column `'satisfied'` is missing data.**\n",
    "\n",
    "For each of the following datasets, label the column `'satisfied'` as being `'MD'`, `'MCAR'`, `'MAR'`, or `'NI'`.\n",
    "\n",
    "1. The dataset consists only of the columns `'customer_id'` and `'satisfied'`.\n",
    "2. The dataset contains the `'customer_id'` of every customer with an account, even if they didn't make a purchase. Also, in this case, you notice everyone who was sent a survey filled it out.\n",
    "3. The dataset contains a column specifying if the user later returned the item.\n",
    "4. The dataset contains a column with the serial number for the item purchased.\n",
    "5. The dataset contains a column with the price of the item purchased.\n",
    "\n",
    "Record your answers in the function `after_purchase` that outputs a list of length 5, containing the values `'MD'`, `'MCAR'`, `'MAR'`, or `'NI'`. For some questions there may be multiple good answers, but there is generally one answer that is \"best\". If you are unsure, ask a tutor, but be prepared to provide justification for whichever answer(s) you think might be right.\n",
    "\n",
    "***Disclaimer:*** We know that this lab has no hidden tests, and so it is possible to just look at the correct answers by running `grader.check`. This is not a good idea ‚Äì you should really think about all of the questions here, since similar questions will be on the Midterm Exam."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def after_purchase():\n",
    "    \"\"\"\n",
    "    Multiple choice response for question 8\n",
    "\n",
    "    >>> out = after_purchase()\n",
    "    >>> ans = ['MD', 'MCAR', 'MAR', 'NI']\n",
    "    >>> len(out) == 5\n",
    "    True\n",
    "    >>> set(out) <= set(ans)\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    '''\n",
    "    Scenario 1: The customer ID column is irrelevant. In the absence of any other information,\n",
    "    our best guess is that customers who were satisfied or felt neutral didn't feel the need to\n",
    "    leave a review, whereas dissatisfied customers may have been more likely to leave a review.\n",
    "    As such, the data is not missing at random, i.e. NI, since the missingness depends on the value\n",
    "    itself.\n",
    "    \n",
    "    Scenario 2: Since we can predict whether or not 'Satisfied' is missing ‚Äì it is missing if and only if\n",
    "    a customer didn't order anything (i.e. 'item' is empty) ‚Äì the data is missing by design.\n",
    "    \n",
    "    Scenario 3: Customers who returned their items are more likely to be dissatisfied than those\n",
    "    who didn't return their items, and dissatisfied customers are more likely to not leave a review.\n",
    "    As such, the missingness depends on the 'Returned?' column, so the data is missing at random.\n",
    "    \n",
    "    Scenario 4: The serial number, like the customer ID, is just some random string which is irrelevant.\n",
    "    This is the same as Scenario 1.\n",
    "    \n",
    "    Scenario 5: Similar to Scenario 3. Perhaps the more expensive items are better, leading to customers\n",
    "    being more satisfied and less likely to leave a review.\n",
    "    '''\n",
    "    out = ['NI', 'MD', 'MAR', 'NI', 'MAR']  # MAR for index=3 as well\n",
    "\n",
    "    return out\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q8_out = after_purchase()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'output length should be 5'\n",
    "\"\"\" # END TEST CONFIG\n",
    "len(q8_out) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'output contains answers other than the specified options'\n",
    "\"\"\" # END TEST CONFIG\n",
    "set(q8_out) <= set(['MD', 'MCAR', 'MAR', 'NI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 1: reviewers are more likely to review when dissatified'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[0] == 'NI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 1: partial; one column must be NI or MCAR'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[0] in ['NI', 'MCAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 2: not the optimal answer'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[1] == 'MD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: .5\n",
    "failure_message: 'sub-question 2: partial'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[1] in ['MD', 'MAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 3: not the optimal answer'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[2] == 'MAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: .5\n",
    "failure_message: 'sub-question 3: partial'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[2] in ['NI', 'MAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 4: not the optimal answer, how does having a serial number affect missingness?'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[3] in ['NI', 'MAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: .5\n",
    "failure_message: 'sub-question 4: partial'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[3] in ['NI', 'MAR', 'MCAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 5: not the optimal answer'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[4] == 'MAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: .5\n",
    "failure_message: 'sub-question 5: partial'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q8_out[4] in ['MAR', 'NI', 'MCAR']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: q9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 ‚Äì Miscellaneous missingness questions üïµÔ∏è\n",
    "\n",
    "In each of the following scenarios, choose the best answer out of the missingness types: `'MD'`, `'MCAR'`, `'MAR'`, and `'NI'`. Store your answers in a list of length 5, and have the function `multiple_choice` return that list.\n",
    "\n",
    "1. UCSD has recently adopted GrubHub as the food pre-ordering app for campus restaurants, so you can order your food ahead of time and stop by before your next class. In a DataFrame of GrubHub app orders, which contains information such as `'restaurant'`, `'name'`, `'items'`, and `'total'`, the column `'delivery_address'` is often missing for UCSD students. Which is the most likely missingness mechanism for this column?\n",
    "\n",
    "\n",
    "2. In a database of student records that records student profile data, such as `'name'`, `'home_address'`, `'ethnicity'`, etc., sometimes the `'middle_name'` column is missing. Which is the most likely missingness mechanism for this column?\n",
    "\n",
    "\n",
    "3. The UCSD Club Basketball team creates a signup sheet for potential new members. The sheet contains the columns `'full_name'`, `'year'`, `'email'`, `'favorite_sports'`, `'number_of_sports_played'`, and `'sports_previously_played'`. The team president notices that many students left the `'sports_previously_played'` column blank. Which is the most likely missingness mechanism for this column?\n",
    "\n",
    "\n",
    "4. After the 2022 Sun God Festival, Associated Students sends out a survey to all students about whether their expectations for the 2022 Sun God Festival were met, with all questions being optional. They notice that many students left the \"Were you satisfied with the 2022 Sun God Festival?\" question blank. Which is the most likely missingness mechanism for answers to this question?\n",
    "\n",
    "\n",
    "5. UCSD has been using a two-factor authentication system, DUO, since October 16th, 2019. When using DUO, all UCSD accounts are assigned a unique code. UCSD's Service Desk, who maintains DUO, has a database that stores each user's code and their phone number, which users must provide when they sign up for DUO. They notice that many phone numbers are missing. Which is the most likely missingness mechanism for phone numbers?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": [
     "to-py"
    ]
   },
   "outputs": [],
   "source": [
    "def multiple_choice():\n",
    "    \"\"\"\n",
    "    Multiple choice response for question 9\n",
    "\n",
    "    >>> out = multiple_choice()\n",
    "    >>> ans = ['MD', 'MCAR', 'MAR', 'NI']\n",
    "    >>> len(out) == 5\n",
    "    True\n",
    "    >>> set(out) <= set(ans)\n",
    "    True\n",
    "    >>> out[1] in ans\n",
    "    True\n",
    "    \"\"\"\n",
    "    # BEGIN SOLUTION\n",
    "    '''\n",
    "    Scenario 1: The other columns may provide some information \n",
    "    as to whether or not a delivery address is missing. For example,\n",
    "    some restaurants may only provide pickup, and hence won't need to\n",
    "    note down a delivery address.\n",
    "    \n",
    "    Scenario 2: The other columns provide some information as to\n",
    "    whether or not a middle name will be missing. Some ethnicities are \n",
    "    less likely to use middle names, for example. Since the missingness\n",
    "    could depend on other columns but not on the missing values themselves,\n",
    "    the correct mechanism is MAR.\n",
    "    \n",
    "    Scenario 3: If a student hasn't played any previous sports,\n",
    "    there is no reason for them to answer this question. It is hence\n",
    "    missing by design. (You could maybe argue that maybe some people were\n",
    "    embarassed by the previous sports they played, and hence it is NI.)\n",
    "    \n",
    "    Scenario 4: People who were dissatisfied are less likely to leave\n",
    "    a response. The missingness depends on the missing value itself,\n",
    "    so it is NI.\n",
    "    \n",
    "    Scenario 5: Since everyone is required to enter a phone number when\n",
    "    signing up for DUO, and the other columns provide no information on\n",
    "    phone numbers, the most likely explanation is that phone numbers\n",
    "    were dropped from the dataset at random.\n",
    "    '''\n",
    "    out = ['MAR', 'MAR', 'MD', 'NI', 'MCAR']\n",
    "\n",
    "    return out\n",
    "    # END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q9_out = multiple_choice()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'output length should be 5'\n",
    "\"\"\" # END TEST CONFIG\n",
    "len(q9_out) == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: false\n",
    "points: 1\n",
    "failure_message: 'output contains answers other than the specified options'\n",
    "\"\"\" # END TEST CONFIG\n",
    "set(q9_out) <= set(['MD', 'MCAR', 'MAR', 'NI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 1'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[0] == 'MAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 1: partial'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[0] in ['MCAR', 'MAR', 'NI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 2'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[1] in ['MAR', 'NI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 3'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[2] in ['MD', 'MAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 4'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[3] == 'NI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 4: partial'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[3] in ['NI', 'MCAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 5'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[4] == 'MCAR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # BEGIN TEST CONFIG\n",
    "hidden: true\n",
    "points: 1\n",
    "failure_message: 'sub-question 5: partial'\n",
    "\"\"\" # END TEST CONFIG\n",
    "q9_out[4] in ['MAR', 'NI', 'MCAR']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done! üèÅ\n",
    "\n",
    "Submit your `lab.py` file to Gradescope. Note that you only need to submit the `lab.py` file; this notebook should not be uploaded.\n",
    "\n",
    "Before submitting, you should ensure that all of your work is in the `lab.py` file. You can do this by running the doctests below, which will verify that your work passes the public tests **and** that your work is in the `lab.py` file. Run the cell below; you should see no output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/nix/store/x0fw0l4d6zwgfdwbpp23iwhm3c6a1hh3-python3-3.9.6/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n",
      "    return _run_code(code, main_globals, None,\r\n",
      "  File \"/nix/store/x0fw0l4d6zwgfdwbpp23iwhm3c6a1hh3-python3-3.9.6/lib/python3.9/runpy.py\", line 87, in _run_code\r\n",
      "    exec(code, run_globals)\r\n",
      "  File \"/nix/store/x0fw0l4d6zwgfdwbpp23iwhm3c6a1hh3-python3-3.9.6/lib/python3.9/doctest.py\", line 2793, in <module>\r\n",
      "    sys.exit(_test())\r\n",
      "  File \"/nix/store/x0fw0l4d6zwgfdwbpp23iwhm3c6a1hh3-python3-3.9.6/lib/python3.9/doctest.py\", line 2781, in _test\r\n",
      "    m = __import__(filename[:-3])\r\n",
      "ModuleNotFoundError: No module named 'lab'\r\n"
     ]
    }
   ],
   "source": [
    "!python -m doctest lab.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, `grader.check_all()` will verify that your work passes the public tests."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
